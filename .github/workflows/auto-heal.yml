# =============================================================================
# Auto-Heal — Detect CI failures, diagnose root causes, and create fix issues
#
# Triggers after any CI Test & Release run completes with failure on main.
#
# Key features:
#   1. DIAGNOSTIC ANALYSIS — runs failing tests locally to categorize errors
#   2. ATTEMPT TRACKING — tracks how many times a fix has been tried
#   3. ESCALATION — after 3 failed attempts, unassigns copilot and pings human
#   4. STRATEGY GUIDANCE — tells the agent WHERE to fix based on error category
#   5. ANTI-REPEAT — explicitly forbids repeating a failed approach
#
# Guard rails:
#   - Only triggers on main branch failures (not PR branches)
#   - Dedup: updates existing issue instead of creating duplicates
#   - Non-fatal copilot assignment (issue created regardless)
# =============================================================================
name: Auto-Heal Test Failures

on:
  workflow_run:
    workflows: ["CI Test & Release"]
    types: [completed]

permissions:
  issues: write
  contents: read
  actions: read

jobs:
  check-and-heal:
    name: "Detect, Diagnose & Dispatch Fix"
    if: >-
      github.event.workflow_run.conclusion == 'failure' &&
      github.event.workflow_run.head_branch == 'main'
    runs-on: ubuntu-latest
    steps:
      # ── 0. Checkout code (needed for diagnostic test runs) ───────────
      - name: "Checkout repository"
        uses: actions/checkout@v4

      - name: "Set up Python"
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: "Install test dependencies"
        run: pip install pytest requests responses --quiet

      # ── 1. Parse failure logs ────────────────────────────────────────
      - name: "Download and parse CI failure logs"
        id: logs
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          RUN_ID: ${{ github.event.workflow_run.id }}
        run: |
          echo "Fetching logs for CI run $RUN_ID"

          gh run view "$RUN_ID" \
            --repo "${{ github.repository }}" \
            --log-failed > /tmp/failed_logs.txt 2>/dev/null || true

          LOG_SIZE=$(wc -c < /tmp/failed_logs.txt | tr -d ' ')
          echo "Log size: $LOG_SIZE bytes"

          if [[ "$LOG_SIZE" -lt 10 ]]; then
            echo "No logs retrieved — workflow may have been cancelled"
            echo "total_failures=0" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # ── Extract pytest failures ──
          grep -oP '\S+\.py::\S+ FAILED' /tmp/failed_logs.txt \
            | sed 's/ FAILED$//' \
            > /tmp/pytest_failures.txt 2>/dev/null || true
          PYTEST_COUNT=$(wc -l < /tmp/pytest_failures.txt | tr -d ' ')

          # Strip ANSI color codes for reliable parsing
          sed 's/\x1b\[[0-9;]*m//g' /tmp/failed_logs.txt > /tmp/failed_logs_clean.txt

          PYTEST_SUMMARY=$(grep -oP '[0-9]+ failed, [0-9]+ passed' /tmp/failed_logs_clean.txt | tail -1 || echo "")
          
          # Extract Pester summary if available
          PESTER_SUMMARY=$(grep -oP 'Tests Passed: [0-9]+, Failed: [0-9]+(, Skipped: [0-9]+)?' /tmp/failed_logs_clean.txt | tail -1 || echo "")

          # ── Extract PowerShell test failures ──
          # First try to get count from Pester summary line (most reliable)
          PESTER_FAILED=$(grep -oP 'Tests Passed: [0-9]+, Failed: \K[0-9]+' /tmp/failed_logs_clean.txt | tail -1 || echo "")
          
          # Also extract parse errors (syntax errors)
          PS_PARSE_ERRORS=$(grep -icE "Parse errors in|DuplicateFormalParameter|TermExpected|Missing closing" /tmp/failed_logs_clean.txt || echo "0")
          
          # If we have a Pester summary, use it; otherwise fall back to error markers
          if [[ -n "$PESTER_FAILED" && "$PESTER_FAILED" != "0" ]]; then
            PS_COUNT=$PESTER_FAILED
          else
            # Count ##[error][-] markers (Pester test failures) + parse errors
            PS_MARKERS=$(grep -c '##\[error\]\[-\]' /tmp/failed_logs.txt || echo "0")
            PS_COUNT=$((PS_MARKERS + PS_PARSE_ERRORS))
          fi

          # ── Extract bash syntax errors ──
          grep -iE "syntax error|bash -n.*failed|^not ok |not ok " /tmp/failed_logs_clean.txt \
            > /tmp/bash_errors.txt 2>/dev/null || true
          BASH_COUNT=$(wc -l < /tmp/bash_errors.txt | tr -d ' ')

          TOTAL=$((PYTEST_COUNT + PS_COUNT + BASH_COUNT))
          echo "total_failures=$TOTAL" >> "$GITHUB_OUTPUT"
          echo "pytest_count=$PYTEST_COUNT" >> "$GITHUB_OUTPUT"
          echo "pytest_summary=$PYTEST_SUMMARY" >> "$GITHUB_OUTPUT"
          echo "pester_summary=$PESTER_SUMMARY" >> "$GITHUB_OUTPUT"
          echo "ps_count=$PS_COUNT" >> "$GITHUB_OUTPUT"
          echo "bash_count=$BASH_COUNT" >> "$GITHUB_OUTPUT"
          echo "run_id=$RUN_ID" >> "$GITHUB_OUTPUT"

          # Extract unique affected domains
          DOMAINS=$(cat /tmp/pytest_failures.txt 2>/dev/null \
            | grep -oP '^\S+?(?=/)' \
            | sort -u | head -20 \
            | tr '\n' ',' | sed 's/,$//')
          echo "domains=${DOMAINS:-unknown}" >> "$GITHUB_OUTPUT"

          # Extract unique test FILE paths (for scoped re-run)
          sed 's/::.*$//' /tmp/pytest_failures.txt \
            | sort -u \
            > /tmp/pytest_failing_files.txt 2>/dev/null || true

          echo "Parsed: pytest=$PYTEST_COUNT ps=$PS_COUNT bash=$BASH_COUNT total=$TOTAL"

      # ── 2. Diagnostic analysis — run failing tests to categorize ─────
      - name: "Run diagnostic analysis"
        id: diagnose
        if: steps.logs.outputs.total_failures != '0'
        run: |
          echo "Running diagnostic analysis on failing tests..."

          DIAG_FILE="/tmp/diagnostic_report.md"
          : > "$DIAG_FILE"

          URL_MISMATCH=0
          ENCODING_ERROR=0
          FIXTURE_ERROR=0
          IMPORT_ERROR=0
          ASSERTION_ERROR=0
          OTHER_ERROR=0

          # Collect per-file details into a separate file
          : > /tmp/diag_details.txt

          while IFS= read -r test_file; do
            [ -z "$test_file" ] && continue
            [ ! -f "$test_file" ] && continue

            # Run the single test file with short traceback
            OUTPUT=$(python -m pytest "$test_file" --tb=short -q 2>&1 || true)
            FAILED_COUNT=$(echo "$OUTPUT" | grep -cP 'FAILED|ERROR' || true)

            if [ "$FAILED_COUNT" -eq 0 ]; then
              continue  # Test passes now (maybe fixed in this commit)
            fi

            # Categorize the failure
            if echo "$OUTPUT" | grep -qiP "URL does not match|ConnectionError.*%7B|url.*mismatch"; then
              URL_MISMATCH=$((URL_MISMATCH + 1))
              CATEGORY="url_mismatch"
            elif echo "$OUTPUT" | grep -qiP "ChunkedEncodingError|IncompleteRead|may not contain content"; then
              ENCODING_ERROR=$((ENCODING_ERROR + 1))
              CATEGORY="encoding_error"
            elif echo "$OUTPUT" | grep -qiP "FileNotFoundError|JSONDecodeError|KeyError.*fixture|No such file"; then
              FIXTURE_ERROR=$((FIXTURE_ERROR + 1))
              CATEGORY="fixture_error"
            elif echo "$OUTPUT" | grep -qiP "ImportError|ModuleNotFoundError|No module named"; then
              IMPORT_ERROR=$((IMPORT_ERROR + 1))
              CATEGORY="import_error"
            elif echo "$OUTPUT" | grep -qiP "AssertionError|assert "; then
              ASSERTION_ERROR=$((ASSERTION_ERROR + 1))
              CATEGORY="assertion_error"
            else
              OTHER_ERROR=$((OTHER_ERROR + 1))
              CATEGORY="other"
            fi

            # Extract the first error detail (5 lines around FAILED/ERROR)
            FIRST_ERROR=$(echo "$OUTPUT" | grep -B2 -A3 "FAILED\|Error\|assert " | head -8)

            {
              printf '#### `%s` — %s\n' "$test_file" "$CATEGORY"
              printf '```\n'
              printf '%s\n' "$FIRST_ERROR"
              printf '```\n\n'
            } >> /tmp/diag_details.txt

          done < /tmp/pytest_failing_files.txt

          # ── Build the diagnostic report ──
          {
            printf '### Diagnostic Report\n\n'
            printf 'Generated: %s\n\n' "$(date -u +%Y-%m-%dT%H:%M:%SZ)"

            printf '#### Failure Categories\n\n'
            printf '| Category | Count | What It Means |\n'
            printf '|----------|-------|---------------|\n'
            [ "$URL_MISMATCH" -gt 0 ]    && printf '| URL mismatch | %d | URL template has unresolved placeholders or wrong host — fix the **source scripts** |\n' "$URL_MISMATCH"
            [ "$ENCODING_ERROR" -gt 0 ]  && printf '| Encoding error | %d | Mock returns body for HTTP 204/DELETE — fix the **test mocks** |\n' "$ENCODING_ERROR"
            [ "$FIXTURE_ERROR" -gt 0 ]   && printf '| Fixture error | %d | Missing or malformed fixture JSON — fix **tests/fixtures/** |\n' "$FIXTURE_ERROR"
            [ "$IMPORT_ERROR" -gt 0 ]    && printf '| Import error | %d | Missing module or __init__.py — fix **package structure** |\n' "$IMPORT_ERROR"
            [ "$ASSERTION_ERROR" -gt 0 ] && printf '| Assertion error | %d | Test expectation wrong — fix **test assertions** or **implementation** |\n' "$ASSERTION_ERROR"
            [ "$OTHER_ERROR" -gt 0 ]     && printf '| Other | %d | Uncategorized — see per-file details below |\n' "$OTHER_ERROR"
            printf '\n'

            # Determine dominant category and recommend strategy
            DOMINANT="other"; MAX=0
            [ "$URL_MISMATCH"    -gt "$MAX" ] && MAX=$URL_MISMATCH    && DOMINANT="url_mismatch"
            [ "$ENCODING_ERROR"  -gt "$MAX" ] && MAX=$ENCODING_ERROR  && DOMINANT="encoding_error"
            [ "$FIXTURE_ERROR"   -gt "$MAX" ] && MAX=$FIXTURE_ERROR   && DOMINANT="fixture_error"
            [ "$IMPORT_ERROR"    -gt "$MAX" ] && MAX=$IMPORT_ERROR    && DOMINANT="import_error"
            [ "$ASSERTION_ERROR" -gt "$MAX" ] && MAX=$ASSERTION_ERROR && DOMINANT="assertion_error"

            printf '#### Recommended Fix Strategy\n\n'
            case "$DOMINANT" in
              url_mismatch)
                printf '**Dominant issue: URL mismatch (%d failures)**\n\n' "$MAX"
                printf 'The implementation scripts build URLs with unresolved template variables (e.g., `{service}`, `{instance}`).\n\n'
                printf '**Fix the SOURCE SCRIPTS** (`.py`, `.ps1`, `.sh`) — NOT the tests.\n\n'
                printf 'Search: `grep -r "{service}" --include="*.py" <Domain>/`\n\n'
                printf 'Replace the placeholder with the correct hostname (usually `dev.azure.com`).\n'
                printf 'Also fix the YAML definitions in `_generator/definitions/` so regeneration stays clean.\n'
                ;;
              encoding_error)
                printf '**Dominant issue: ChunkedEncodingError (%d failures)**\n\n' "$MAX"
                printf 'Test mocks return a JSON body for HTTP 204 No Content responses.\n\n'
                printf '**Fix the TEST FILES** — change `responses.add(method, url, json=fixture, status=204)` to `responses.add(method, url, status=204)`.\n\n'
                printf 'Also fix the generator template `_generator/templates/python_test.py` to prevent this for future generations.\n'
                ;;
              fixture_error)
                printf '**Dominant issue: Fixture/data errors (%d failures)**\n\n' "$MAX"
                printf '**Fix the fixture files** in `tests/fixtures/` — add missing keys or regenerate.\n\n'
                printf 'Also check `_generator/templates/python_test.py` for fixture generation logic.\n'
                ;;
              import_error)
                printf '**Dominant issue: Import errors (%d failures)**\n\n' "$MAX"
                printf '**Fix the package structure** — ensure `__init__.py` exists in each directory.\n\n'
                printf 'Check `sys.path` setup in test files.\n'
                ;;
              assertion_error)
                printf '**Dominant issue: Assertion errors (%d failures)**\n\n' "$MAX"
                printf 'Test expectations do not match implementation behavior.\n\n'
                printf '**Compare** what the implementation returns vs what the test asserts.\n\n'
                printf 'If 10+ files have the same mismatch pattern, fix the **generator template**.\n'
                ;;
              *)
                printf '**Mixed failure types** — review each file individually.\n'
                ;;
            esac

            printf '\n#### Per-File Error Details\n\n'
          } > "$DIAG_FILE"

          # Append per-file details (truncated to 50 files max)
          head -500 /tmp/diag_details.txt >> "$DIAG_FILE"

          # Output counts for downstream steps
          echo "url_mismatch=$URL_MISMATCH" >> "$GITHUB_OUTPUT"
          echo "encoding_error=$ENCODING_ERROR" >> "$GITHUB_OUTPUT"
          echo "fixture_error=$FIXTURE_ERROR" >> "$GITHUB_OUTPUT"
          echo "import_error=$IMPORT_ERROR" >> "$GITHUB_OUTPUT"
          echo "assertion_error=$ASSERTION_ERROR" >> "$GITHUB_OUTPUT"
          echo "other_error=$OTHER_ERROR" >> "$GITHUB_OUTPUT"
          echo "dominant=$DOMINANT" >> "$GITHUB_OUTPUT"

          STILL_FAILING=$((URL_MISMATCH + ENCODING_ERROR + FIXTURE_ERROR + IMPORT_ERROR + ASSERTION_ERROR + OTHER_ERROR))
          echo "still_failing=$STILL_FAILING" >> "$GITHUB_OUTPUT"

          echo "Diagnostic complete: $STILL_FAILING test files still failing"
          echo "  url_mismatch=$URL_MISMATCH encoding=$ENCODING_ERROR fixture=$FIXTURE_ERROR"
          echo "  import=$IMPORT_ERROR assertion=$ASSERTION_ERROR other=$OTHER_ERROR"

      # ── 3. Check for existing issue + count attempts ─────────────────
      - name: "Check for existing auto-heal issue and count attempts"
        id: dedup
        if: steps.logs.outputs.total_failures != '0'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Find open auto-heal issues
          OPEN_ISSUES=$(gh issue list \
            --repo "${{ github.repository }}" \
            --label "auto-heal" \
            --state open \
            --json number,body \
            --jq '.')
          OPEN_COUNT=$(echo "$OPEN_ISSUES" | jq 'length')
          echo "Open auto-heal issues: $OPEN_COUNT"
          echo "open_count=$OPEN_COUNT" >> "$GITHUB_OUTPUT"

          if [[ "$OPEN_COUNT" -gt 0 ]]; then
            EXISTING_NUM=$(echo "$OPEN_ISSUES" | jq -r '.[0].number')
            EXISTING_BODY=$(echo "$OPEN_ISSUES" | jq -r '.[0].body')

            # Extract attempt number from issue body or comments
            # Format: "**Attempt: N**"
            ATTEMPT=$(echo "$EXISTING_BODY" | grep -oP '\*\*Attempt: \K\d+' || echo "1")

            # Also check comments for higher attempt numbers
            COMMENT_ATTEMPT=$(gh issue view "$EXISTING_NUM" \
              --repo "${{ github.repository }}" \
              --json comments \
              --jq '.comments[].body' 2>/dev/null \
              | grep -oP '\*\*Attempt: \K\d+' \
              | sort -n | tail -1 || echo "0")

            # Use the higher of body vs comment attempt numbers
            if [[ "$COMMENT_ATTEMPT" -gt "$ATTEMPT" ]]; then
              ATTEMPT=$COMMENT_ATTEMPT
            fi

            echo "existing_issue=$EXISTING_NUM" >> "$GITHUB_OUTPUT"
            echo "attempt=$ATTEMPT" >> "$GITHUB_OUTPUT"
            echo "Found existing issue #$EXISTING_NUM at attempt $ATTEMPT"
          else
            echo "existing_issue=" >> "$GITHUB_OUTPUT"
            echo "attempt=0" >> "$GITHUB_OUTPUT"
          fi

      # ── 4a. ESCALATE if stuck (attempt >= 3) ─────────────────────────
      - name: "Escalate — agent loop detected"
        if: >-
          steps.logs.outputs.total_failures != '0' &&
          steps.dedup.outputs.open_count != '0' &&
          steps.dedup.outputs.attempt != '' &&
          steps.dedup.outputs.attempt >= 3
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ADMIN_PAT: ${{ secrets.ADMIN_PAT }}
        run: |
          ISSUE_NUM="${{ steps.dedup.outputs.existing_issue }}"
          ATTEMPT="${{ steps.dedup.outputs.attempt }}"
          REPO="${{ github.repository }}"

          echo "::warning::Auto-heal stuck after $ATTEMPT attempts. Escalating issue #$ISSUE_NUM."

          # Ensure escalation label exists
          gh label create "escalation" \
            --repo "$REPO" \
            --description "Auto-heal stuck — needs human review" \
            --color "B60205" \
            2>/dev/null || true

          # Build escalation comment
          cat > /tmp/escalation.md <<ESCEOF
          ## :warning: Auto-Heal Escalation — Agent Loop Detected

          The same failures have persisted through **${ATTEMPT} consecutive fix attempts**.
          The automated agent is not making progress. **The loop has been broken.**

          ### What Happened

          1. CI detected test failures on \`main\`
          2. Auto-heal created this issue and assigned copilot
          3. Copilot attempted a fix, but CI still failed after merge
          4. This cycle repeated ${ATTEMPT} times without resolution

          ### Diagnostic Report (Latest Run)

          ESCEOF

          # Append diagnostic report
          if [[ -f /tmp/diagnostic_report.md ]]; then
            cat /tmp/diagnostic_report.md >> /tmp/escalation.md
          fi

          cat >> /tmp/escalation.md <<'ESCEOF2'

          ### Actions Taken

          - [x] Copilot has been **unassigned** to stop the loop
          - [x] `escalation` label added
          - [ ] **Human review required** — see diagnostic report above

          ### What To Do

          1. Read the **Recommended Fix Strategy** in the diagnostic report
          2. If the dominant category is `url_mismatch`: fix the **source scripts**, not the tests
          3. If the dominant category is `encoding_error`: fix the **test mocks** (204 status)
          4. If 10+ files share the same bug pattern: fix the **generator template** and regenerate
          5. After fixing, close this issue — auto-heal will create a fresh one if failures remain

          /cc @tdevere
          ESCEOF2

          sed -i 's/^          //' /tmp/escalation.md

          gh issue comment "$ISSUE_NUM" \
            --repo "$REPO" \
            --body-file /tmp/escalation.md

          # Add escalation label
          gh issue edit "$ISSUE_NUM" \
            --repo "$REPO" \
            --add-label "escalation"

          # Unassign copilot to break the loop
          if [[ -n "$ADMIN_PAT" ]]; then
            curl -s -o /dev/null \
              -X DELETE \
              -H "Authorization: token $ADMIN_PAT" \
              -H "Accept: application/vnd.github+json" \
              -H "X-GitHub-Api-Version: 2022-11-28" \
              "https://api.github.com/repos/$REPO/issues/$ISSUE_NUM/assignees" \
              -d '{"assignees":["copilot"]}' || true
            echo "Copilot unassigned from issue #$ISSUE_NUM"
          fi

          echo "Escalation complete. Agent loop broken."

      # ── 4b. UPDATE existing issue with retry context (attempt < 3) ───
      - name: "Update existing issue with retry context"
        if: >-
          steps.logs.outputs.total_failures != '0' &&
          steps.dedup.outputs.open_count != '0' &&
          steps.dedup.outputs.attempt != '' &&
          steps.dedup.outputs.attempt < 3 &&
          steps.diagnose.outputs.still_failing != '0'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ISSUE_NUM="${{ steps.dedup.outputs.existing_issue }}"
          ATTEMPT="${{ steps.dedup.outputs.attempt }}"
          NEXT_ATTEMPT=$((ATTEMPT + 1))
          STILL_FAILING="${{ steps.diagnose.outputs.still_failing }}"
          DOMINANT="${{ steps.diagnose.outputs.dominant }}"
          RUN_ID="${{ steps.logs.outputs.run_id }}"

          cat > /tmp/retry_comment.md <<RETRYEOF
          ## :arrows_counterclockwise: Previous Fix Did Not Resolve All Failures

          **Attempt: ${NEXT_ATTEMPT}** | **Still failing: ${STILL_FAILING} test files** | [CI Run](https://github.com/${{ github.repository }}/actions/runs/${RUN_ID})

          > :stop_sign: **Do NOT repeat your previous approach.** It did not work.
          > If you modified tests last time, try modifying the **source scripts** instead (or vice versa).
          > If you modified individual files last time, consider fixing the **generator template** instead.

          RETRYEOF

          # Append diagnostic report
          if [[ -f /tmp/diagnostic_report.md ]]; then
            cat /tmp/diagnostic_report.md >> /tmp/retry_comment.md
          fi

          {
            printf '\n### Scoped Test Command\n\n'
            printf 'Run **only** these failing tests:\n'
            printf '```bash\n'
            printf 'python -m pytest --tb=long -q \\\n'
            while IFS= read -r f; do
              [ -z "$f" ] && continue
              printf '  %s \\\n' "$f"
            done < /tmp/pytest_failing_files.txt
            printf '  2>&1 | tail -60\n'
            printf '```\n\n'
            printf '### Rules for Attempt %d\n\n' "$NEXT_ATTEMPT"
            printf '1. **Read the Recommended Fix Strategy above** before writing any code\n'
            printf '2. **Do NOT re-run the full test suite** — only the files listed above\n'
          } >> /tmp/retry_comment.md

          case "$DOMINANT" in
            url_mismatch)
              printf '3. The issue is in **source scripts** (`.py`, `.ps1`, `.sh`) — look for unresolved `{service}` or `{instance}` placeholders\n' >> /tmp/retry_comment.md
              printf '4. Also fix the YAML definitions in `_generator/definitions/`\n' >> /tmp/retry_comment.md
              ;;
            encoding_error)
              printf '3. The issue is in **test mocks** — `responses.add()` calls with `json=fixture` for 204/DELETE endpoints\n' >> /tmp/retry_comment.md
              printf '4. Remove the `json=` argument for any mock with `status=204`\n' >> /tmp/retry_comment.md
              ;;
            fixture_error)
              printf '3. Check the fixture JSON files in `tests/fixtures/` directories\n' >> /tmp/retry_comment.md
              printf '4. Regenerate fixtures or add missing keys\n' >> /tmp/retry_comment.md
              ;;
            *)
              printf '3. Review the per-file error details above to understand each failure\n' >> /tmp/retry_comment.md
              printf '4. If 10+ files have the same pattern, fix the **generator template** in `_generator/templates/`\n' >> /tmp/retry_comment.md
              ;;
          esac

          printf '\n**Attempt: %d**\n' "$NEXT_ATTEMPT" >> /tmp/retry_comment.md

          sed -i 's/^          //' /tmp/retry_comment.md

          gh issue comment "$ISSUE_NUM" \
            --repo "${{ github.repository }}" \
            --body-file /tmp/retry_comment.md

          echo "Updated issue #$ISSUE_NUM with attempt $NEXT_ATTEMPT context"

      # ── 4c. CREATE new healing issue (no existing open issue) ────────
      - name: "Create new fix issue"
        id: create
        if: >-
          steps.logs.outputs.total_failures != '0' &&
          steps.dedup.outputs.open_count == '0' &&
          steps.diagnose.outputs.still_failing != '0'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          RUN_ID="${{ steps.logs.outputs.run_id }}"
          TOTAL="${{ steps.logs.outputs.total_failures }}"
          PYTEST_COUNT="${{ steps.logs.outputs.pytest_count }}"
          PYTEST_SUMMARY="${{ steps.logs.outputs.pytest_summary }}"
          PESTER_SUMMARY="${{ steps.logs.outputs.pester_summary }}"
          PS_COUNT="${{ steps.logs.outputs.ps_count }}"
          BASH_COUNT="${{ steps.logs.outputs.bash_count }}"
          DOMAINS="${{ steps.logs.outputs.domains }}"
          STILL_FAILING="${{ steps.diagnose.outputs.still_failing }}"

          # Ensure labels exist
          gh label create "auto-heal" \
            --repo "${{ github.repository }}" \
            --description "Auto-generated test failure fix" \
            --color "D93F0B" \
            2>/dev/null || true

          TITLE="[AUTO-HEAL] Fix $TOTAL test failure(s) in ${DOMAINS} from CI run #${RUN_ID}"

          # ── Build issue body in a file ──
          cat > /tmp/issue_body.md <<HEADEREOF
          ## Task: Fix test failures detected in CI

          ### Context

          The CI pipeline on \`main\` has **${TOTAL} failure(s)**.
          ${PYTEST_SUMMARY:+- pytest: $PYTEST_SUMMARY}
          ${PESTER_SUMMARY:+- Pester: $PESTER_SUMMARY}
          These must be fixed to keep the project healthy.

          **CI Run:** https://github.com/${{ github.repository }}/actions/runs/${RUN_ID}

          | Framework | Failures |
          |-----------|----------|
          | pytest | ${PYTEST_COUNT} |
          | PowerShell | ${PS_COUNT} |
          | Bash/bats | ${BASH_COUNT} |

          HEADEREOF

          # Append diagnostic report (categorized errors + strategy)
          if [[ -f /tmp/diagnostic_report.md ]]; then
            cat /tmp/diagnostic_report.md >> /tmp/issue_body.md
          fi

          # Build scoped test command
          {
            printf '\n### Scoped Test Command\n\n'
            printf 'Run **only** these failing tests — do NOT re-run the full suite (7000+ other tests pass):\n'
            printf '```bash\n'
            printf 'python -m pytest --tb=long -q \\\n'
            while IFS= read -r f; do
              [ -z "$f" ] && continue
              printf '  %s \\\n' "$f"
            done < /tmp/pytest_failing_files.txt
            printf '  2>&1 | tail -60\n'
            printf '```\n'
          } >> /tmp/issue_body.md

          cat >> /tmp/issue_body.md <<'INSTREOF'

          ### Instructions

          1. **Read the Diagnostic Report above FIRST** — it categorizes each failure and tells you WHERE to fix
          2. **Follow the "Recommended Fix Strategy"** — it tells you whether to fix source scripts, test mocks, fixtures, or generator templates
          3. **If 10+ files have the same bug pattern**: fix the generator template in `_generator/templates/`, then regenerate affected files
          4. **Verify ONLY the listed tests pass** before committing — use the scoped command above
          5. **Do NOT skip or delete failing tests**
          6. **Do NOT re-run the full test suite** — only run the specifically listed failing files

          ### Acceptance Criteria

          - [ ] All listed pytest tests pass (run the scoped command above)
          - [ ] No tests were deleted or skipped to hide failures
          - [ ] If a template was fixed, all regenerated files also pass
          - [ ] Any YAML definition fixes are applied so future regeneration is clean

          **Attempt: 1**

          ### Reference
          See `.github/copilot-instructions.md` for project conventions.
          INSTREOF

          # Dedent heredoc indentation
          sed -i 's/^          //' /tmp/issue_body.md

          ISSUE_URL=$(gh issue create \
            --repo "${{ github.repository }}" \
            --title "$TITLE" \
            --body-file /tmp/issue_body.md \
            --label "auto-heal" \
            --label "bug")

          echo "Created: $ISSUE_URL"
          ISSUE_NUM=$(echo "$ISSUE_URL" | grep -oP '\d+$')
          echo "issue_number=$ISSUE_NUM" >> "$GITHUB_OUTPUT"
          echo "issue_url=$ISSUE_URL" >> "$GITHUB_OUTPUT"

      # ── 5. Assign copilot ────────────────────────────────────────────
      - name: "Assign Copilot to fix issue"
        if: steps.create.outputs.issue_number != ''
        env:
          ADMIN_PAT: ${{ secrets.ADMIN_PAT }}
        run: |
          ISSUE_NUM="${{ steps.create.outputs.issue_number }}"
          REPO="${{ github.repository }}"
          echo "Assigning copilot to issue #$ISSUE_NUM..."

          HTTP_CODE=$(curl -s -o /tmp/assign_resp.json -w "%{http_code}" \
            -X POST \
            -H "Authorization: token $ADMIN_PAT" \
            -H "Accept: application/vnd.github+json" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            "https://api.github.com/repos/$REPO/issues/$ISSUE_NUM/assignees" \
            -d '{"assignees":["copilot"]}')

          # Verify the assignee actually stuck
          ASSIGNEES=$(curl -s \
            -H "Authorization: token $ADMIN_PAT" \
            -H "Accept: application/vnd.github+json" \
            "https://api.github.com/repos/$REPO/issues/$ISSUE_NUM" \
            | python3 -c "import sys,json; print(','.join(a['login'] for a in json.load(sys.stdin).get('assignees',[])))" 2>/dev/null || echo "")

          if echo "$ASSIGNEES" | grep -q "copilot"; then
            echo "Confirmed: copilot assigned to issue #$ISSUE_NUM"
          elif [[ "$HTTP_CODE" -lt 300 ]]; then
            echo "::warning::API returned $HTTP_CODE but copilot not in assignees: $ASSIGNEES"
            echo "Issue was created — assign copilot manually:"
            echo "  ${{ steps.create.outputs.issue_url }}"
          else
            echo "::warning::Assignment returned HTTP $HTTP_CODE"
            cat /tmp/assign_resp.json 2>/dev/null | head -5
            echo "Issue was created — assign copilot manually:"
            echo "  ${{ steps.create.outputs.issue_url }}"
          fi

      # ── 6. Summary ──────────────────────────────────────────────────
      - name: "Summary"
        if: always()
        run: |
          echo "### Auto-Heal Results" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          TOTAL="${{ steps.logs.outputs.total_failures }}"
          ATTEMPT="${{ steps.dedup.outputs.attempt }}"
          STILL="${{ steps.diagnose.outputs.still_failing }}"

          echo "| Metric | Value |" >> "$GITHUB_STEP_SUMMARY"
          echo "|--------|-------|" >> "$GITHUB_STEP_SUMMARY"
          echo "| Failures in CI logs | ${TOTAL:-0} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Still failing (after checkout) | ${STILL:-n/a} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Attempt | ${ATTEMPT:-0} |" >> "$GITHUB_STEP_SUMMARY"

          if [[ "${ATTEMPT:-0}" -ge 3 ]]; then
            echo "| Status | :warning: **ESCALATED** — agent loop broken, human review needed |" >> "$GITHUB_STEP_SUMMARY"
          elif [[ "${TOTAL:-0}" == "0" ]]; then
            echo "| Status | :white_check_mark: No failures detected |" >> "$GITHUB_STEP_SUMMARY"
          elif [[ "${STILL:-0}" == "0" ]]; then
            echo "| Status | :white_check_mark: All failures resolved in current commit |" >> "$GITHUB_STEP_SUMMARY"
          elif [[ -n "${{ steps.create.outputs.issue_url }}" ]]; then
            echo "| Status | :adhesive_bandage: Created fix issue: ${{ steps.create.outputs.issue_url }} |" >> "$GITHUB_STEP_SUMMARY"
          elif [[ "${{ steps.dedup.outputs.open_count }}" != "0" ]]; then
            echo "| Status | :arrows_counterclockwise: Updated existing issue #${{ steps.dedup.outputs.existing_issue }} with attempt $((ATTEMPT + 1)) |" >> "$GITHUB_STEP_SUMMARY"
          fi
